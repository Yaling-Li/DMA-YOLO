# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Common modules ç½‘ç»œç»„ä»¶æ¨¡å—
"""

import logging
import math
import warnings
from copy import copy
from pathlib import Path

import numpy as np
import pandas as pd
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
from typing import Optional
from torch.cuda import amp

#è¿™éƒ¨åˆ†ä¸ºutilçš„å†…å®¹
from utils.datasets import exif_transpose, letterbox
from utils.general import (colorstr, increment_path, make_divisible, non_max_suppression, save_one_box, scale_coords,
                           xyxy2xywh)
from utils.plots import Annotator, colors
from utils.torch_utils import time_sync

from models.GhostV2 import Ghostblockv2

LOGGER = logging.getLogger(__name__)

def autopad(k, p=None):  # kernel, padding
    """# Pad to 'same'
    # ä¸ºsameå·ç§¯æˆ–sameæ± åŒ–è‡ªåŠ¨æ‰©å……
    # é€šè¿‡å·ç§¯æ ¸çš„å¤§å°æ¥è®¡ç®—éœ€è¦çš„paddingä¸ºå¤šå°‘æ‰èƒ½æŠŠtensorè¡¥æˆåŸæ¥çš„å½¢çŠ¶ã€‚
    ç”¨äºConvå‡½æ•°å’ŒClassifyå‡½æ•°ä¸­
       æ ¹æ®å·ç§¯æ ¸å¤§å°kè‡ªåŠ¨è®¡ç®—å·ç§¯æ ¸paddingæ•°ï¼ˆ0å¡«å……ï¼‰
       v5ä¸­åªæœ‰ä¸¤ç§å·ç§¯ï¼š
          1ã€ä¸‹é‡‡æ ·å·ç§¯:conv3x3 s=2 p=k//2=1
          2ã€feature sizeä¸å˜çš„å·ç§¯:conv1x1 s=1 p=k//2=1
       :params k: å·ç§¯æ ¸çš„kernel_size
       :return p: è‡ªåŠ¨è®¡ç®—çš„éœ€è¦padå€¼ï¼ˆ0å¡«å……ï¼‰

    # å¦‚æœkæ˜¯int åˆ™è¿›è¡Œk//2 è‹¥ä¸æ˜¯åˆ™è¿›è¡Œx//2"""
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p

class Conv(nn.Module):
    # æ¶æ„å›¾ä¸­çš„æ ‡å‡†å·ç§¯ï¼š conv+BN+act
    """åœ¨Focusã€Bottleneckã€BottleneckCSPã€C3ã€SPPã€DWConvã€TransformerBlocç­‰æ¨¡å—ä¸­è°ƒç”¨
          Standard convolution  conv+BN+act
          :params c1: è¾“å…¥çš„channelå€¼
          :params c2: è¾“å‡ºçš„channelå€¼
          :params k: å·ç§¯çš„kernel_size
          :params s: å·ç§¯çš„stride
          :params p: å·ç§¯çš„padding  ä¸€èˆ¬æ˜¯None  å¯ä»¥é€šè¿‡autopadè‡ªè¡Œè®¡ç®—éœ€è¦padçš„paddingæ•°
          :params g: å·ç§¯çš„groupsæ•°  =1å°±æ˜¯æ™®é€šçš„å·ç§¯  >1å°±æ˜¯æ·±åº¦å¯åˆ†ç¦»å·ç§¯
          :params act: æ¿€æ´»å‡½æ•°ç±»å‹   Trueå°±æ˜¯SiLU()/Swish   Falseå°±æ˜¯ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
                       ç±»å‹æ˜¯nn.Moduleå°±ä½¿ç”¨ä¼ è¿›æ¥çš„æ¿€æ´»å‡½æ•°ç±»å‹
          """
    # Standard convolution
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        # convè°ƒç”¨nn.Conv2då‡½æ•°ï¼Œpé‡‡ç”¨autopadï¼Œä¸ä½¿ç”¨åç½®bias=False å› ä¸ºä¸‹é¢åšèåˆæ—¶ï¼Œè¿™ä¸ªå·ç§¯çš„biasä¼šè¢«æ¶ˆæ‰ï¼Œ æ‰€ä»¥ä¸ç”¨
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        # å¦‚æœactæ˜¯trueåˆ™ä½¿ç”¨nn.SiLU å¦åˆ™ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    def forward(self, x):# å‰å‘è®¡ç®—ï¼Œç½‘ç»œæ‰§è¡Œçš„é¡ºåºæ˜¯æ ¹æ®forwardå‡½æ•°æ¥å†³å®šçš„
        return self.act(self.bn(self.conv(x)))# å…ˆconvå·ç§¯ç„¶ååœ¨bnæœ€åactæ¿€æ´»

    def forward_fuse(self, x):# å‰å‘èåˆè®¡ç®—
        #ç”¨äºModelç±»çš„fuseå‡½æ•°,èåˆconv+bn åŠ é€Ÿæ¨ç† ä¸€èˆ¬ç”¨äºæµ‹è¯•/éªŒè¯é˜¶æ®µ
        return self.act(self.conv(x))# è¿™é‡Œåªæœ‰å·ç§¯å’Œæ¿€æ´»

class DWConv(Conv):
    # Depth-wise convolution class# è¿™é‡Œçš„æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œä¸»è¦æ˜¯å°†é€šé“æŒ‰è¾“å…¥è¾“å‡ºçš„æœ€å¤§å…¬çº¦æ•°è¿›è¡Œåˆ‡åˆ†ï¼Œåœ¨ä¸åŒçš„ç‰¹å¾å›¾å±‚ä¸Šè¿›è¡Œç‰¹å¾å­¦ä¹ 
    def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)

class Focus(nn.Module):
    # åŸºæœ¬ç»„ä»¶,æ¨¡å‹çš„ä¸€å¼€å§‹ï¼Œå°†è¾“å…¥å›¾åƒå…ˆ slice æˆ4ä»½ï¼Œå†åšconcat
    # è®¾è®¡æ€è·¯ï¼šç†è®ºä¸Šä»é«˜åˆ†è¾¨ç‡å›¾åƒä¸­ï¼Œå‘¨æœŸæ€§çš„æŠ½å‡ºåƒç´ ç‚¹é‡æ„åˆ°ä½åˆ†è¾¨ç‡å›¾åƒä¸­ï¼Œå³å°†å›¾åƒç›¸é‚»çš„å››ä¸ªä½ç½®è¿›è¡Œå †å ï¼Œèšç„¦whç»´åº¦ä¿¡æ¯åˆ°cé€šé“ç©ºï¼Œæé«˜æ¯ä¸ªç‚¹æ„Ÿå—é‡ï¼Œå¹¶å‡å°‘åŸå§‹ä¿¡æ¯çš„ä¸¢å¤±ã€‚è¿™ä¸ªç»„ä»¶å¹¶ä¸æ˜¯ä¸ºäº†å¢åŠ ç½‘ç»œçš„ç²¾åº¦çš„ï¼Œè€Œæ˜¯ä¸ºäº†å‡å°‘è®¡ç®—é‡ï¼Œå¢åŠ é€Ÿåº¦ã€‚
    # Focus wh information into c-space
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)#concatåçš„å·ç§¯
        # self.contract = Contract(gain=2)

    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2) æœ‰ç‚¹åƒåšä¸€ä¸ªä¸‹é‡‡æ ·
        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))
        # return self.conv(self.contract(x))

class Mlp(nn.Module):
    """ MLP as used in Vision Transformer, MLP-Mixer and related networks
    """
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop)
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop2 = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop1(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x

class Bottleneck(nn.Module):
    # æ ‡å‡†Bottlenck ç½‘ç»œæ¶æ„ä¸­çš„bottlenackæ¨¡å— ï¼Œåˆ†ä¸ºTrueå’ŒFalseæ®‹å·®å—ç»“æ„   ç”±ä¸€äº› 1x1convã€3x3convã€æ®‹å·®å—ç»„æˆ
    """åœ¨BottleneckCSPå’Œyolo.pyçš„parse_modelä¸­è°ƒç”¨
            Standard bottleneck  Conv+Conv+shortcut
            :params c1: ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å…¥channel
            :params c2: ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å‡ºchannel
            :params shortcut: bool æ˜¯å¦æœ‰shortcutè¿æ¥ é»˜è®¤æ˜¯True
            :params g: å·ç§¯åˆ†ç»„çš„ä¸ªæ•°  =1å°±æ˜¯æ™®é€šå·ç§¯  >1å°±æ˜¯æ·±åº¦å¯åˆ†ç¦»å·ç§¯
            :params e: expansion ratio  e*c2å°±æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºchannel=ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥channel
            """
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))

class BottleneckCSP(nn.Module):
    # å‡ ä¸ªæ ‡å‡†Bottleneckçš„å †å +å‡ ä¸ªæ ‡å‡†å·ç§¯å±‚
    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        self.act = nn.SiLU()
        # å åŠ næ¬¡Bottleneck
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        y1 = self.cv3(self.m(self.cv1(x)))
        y2 = self.cv2(x)
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))

class C3(nn.Module):
    # C3æ¨¡å—å’ŒBottleneckCSPæ¨¡å—ç±»ä¼¼ï¼Œä½†æ˜¯å°‘äº†ä¸€ä¸ªConvæ¨¡å— ä¸€ç§ç®€åŒ–ç‰ˆçš„BottleneckCSPï¼Œå› ä¸ºé™¤äº†Bottleneckéƒ¨åˆ†åªæœ‰3ä¸ªå·ç§¯ï¼Œå¯ä»¥å‡å°‘å‚æ•°ï¼Œæ‰€ä»¥å–åC3
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        """åœ¨C3TRæ¨¡å—å’Œyolo.pyçš„parse_modelæ¨¡å—è°ƒç”¨
               CSP Bottleneck with 3 convolutions
               :params c1: æ•´ä¸ªBottleneckCSPçš„è¾“å…¥channel
               :params c2: æ•´ä¸ªBottleneckCSPçš„è¾“å‡ºchannel
               :params n: æœ‰nä¸ªBottleneck
               :params shortcut: bool Bottleneckä¸­æ˜¯å¦æœ‰shortcutï¼Œé»˜è®¤True
               :params g: Bottleneckä¸­çš„3x3å·ç§¯ç±»å‹  =1æ™®é€šå·ç§¯  >1æ·±åº¦å¯åˆ†ç¦»å·ç§¯
               :params e: expansion ratio c2xe=ä¸­é—´å…¶ä»–æ‰€æœ‰å±‚çš„å·ç§¯æ ¸ä¸ªæ•°/ä¸­é—´æ‰€æœ‰å±‚çš„è¾“å…¥è¾“å‡ºchannelæ•°
               """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))
        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])
        # å®éªŒæ€§ crossconv

    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))

class C3TR(C3):
    # C3 module with TransformerBlock() è¿™éƒ¨åˆ†æ˜¯æ ¹æ®ä¸Šé¢çš„C3ç»“æ„æ”¹ç¼–è€Œæ¥çš„å°†åŸå…ˆçš„Bottleneckæ›¿æ¢ä¸ºè°ƒç”¨TransformerBlockæ¨¡å—
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)

class C3STR(C3):
    # C3 module with SwinTransformerBlock()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = SwinTransformerBlock(c_, c_, c_//32, n)

class C3SPP(C3):
    # C3 module with SPP()
    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = SPP(c_, c_, k)

class C3Ghost(C3):
    # C3 module with GhostBottleneck()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))

class SPP(nn.Module):
    # SPPçš„æå‡ºå°±æ˜¯ä¸ºäº†è§£å†³CNNè¾“å…¥å›¾åƒå¤§å°å¿…é¡»å›ºå®šçš„é—®é¢˜
    # èåˆäº†å±€éƒ¨ä¸å…¨å±€ç‰¹å¾ï¼ŒåŒæ—¶å…¼é¡¾äº†é¿å…è£å‰ªå›¾åƒçš„ä½œç”¨
    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729
    def __init__(self, c1, c2, k=(5, 9, 13)):
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)# 1*1çš„å·ç§¯
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))

class ASPP(nn.Module):
    # Atrous Spatial Pyramid Pooling (ASPP) layer
    def __init__(self, c1, c2, k=(5, 9, 13)):
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.m = nn.ModuleList([nn.Conv2d(c_, c_, kernel_size=3, stride=1, padding=(x-1)//2, dilation=(x-1)//2, bias=False) for x in k])
        self.cv2 = Conv(c_ * (len(k) + 2), c2, 1, 1)

    def forward(self, x):
        x = self.cv1(x)
        return self.cv2(torch.cat([x]+ [self.maxpool(x)] + [m(x) for m in self.m] , 1))

class SPPF(nn.Module):
    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher
    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            y1 = self.m(x)
            y2 = self.m(y1)
            return self.cv2(torch.cat([x, y1, y2, self.m(y2)], 1))

class ChannelAttentionModule(nn.Module):
    """
            :params: in_planes è¾“å…¥æ¨¡å—çš„feature mapçš„channel
            :params: ratio é™ç»´/å‡ç»´å› å­
            é€šé“æ³¨æ„åŠ›åˆ™æ˜¯å°†ä¸€ä¸ªé€šé“å†…çš„ä¿¡æ¯ç›´æ¥è¿›è¡Œå…¨å±€å¤„ç†ï¼Œå®¹æ˜“å¿½ç•¥é€šé“å†…çš„ä¿¡æ¯äº¤äº’
            """

    def __init__(self, c1, reduction=16):
        super(ChannelAttentionModule, self).__init__()
        mid_channel = c1 // reduction
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # å¹³å‡æ± åŒ–ï¼Œæ˜¯å–æ•´ä¸ªchannelæ‰€æœ‰å…ƒç´ çš„å‡å€¼ [3,5,5] => [3,1,1]
        self.max_pool = nn.AdaptiveMaxPool2d(1)  # æœ€å¤§æ± åŒ–ï¼Œæ˜¯å–æ•´ä¸ªchannelæ‰€æœ‰å…ƒç´ çš„æœ€å¤§å€¼[3,5,5] => [3,1,1]
        self.shared_MLP = nn.Sequential(
            nn.Linear(in_features=c1, out_features=mid_channel),
            nn.ReLU(),
            nn.Linear(in_features=mid_channel, out_features=c1)
        )
        self.sigmoid = nn.Sigmoid()
        # self.act=SiLU()

    def forward(self, x):
        avgout = self.shared_MLP(self.avg_pool(x).view(x.size(0), -1)).unsqueeze(2).unsqueeze(3)
        maxout = self.shared_MLP(self.max_pool(x).view(x.size(0), -1)).unsqueeze(2).unsqueeze(3)
        return self.sigmoid(avgout + maxout)

class SpatialAttentionModule(nn.Module):
    # å¯¹ç©ºé—´æ³¨æ„åŠ›æ¥è¯´ï¼Œç”±äºå°†æ¯ä¸ªé€šé“ä¸­çš„ç‰¹å¾éƒ½åšåŒç­‰å¤„ç†ï¼Œå®¹æ˜“å¿½ç•¥é€šé“é—´çš„ä¿¡æ¯äº¤äº’
    def __init__(self):
        super(SpatialAttentionModule, self).__init__()
        # è¿™é‡Œè¦ä¿æŒå·ç§¯åçš„featureå°ºåº¦ä¸å˜ï¼Œå¿…é¡»è¦padding=kernel_size//2
        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)
        # self.act=SiLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):  # è¾“å…¥x = [b, c, 56, 56]
        avgout = torch.mean(x, dim=1, keepdim=True)  # avg_out = [b, 1, 56, 56]  æ±‚xçš„æ¯ä¸ªåƒç´ åœ¨æ‰€æœ‰channelç›¸åŒä½ç½®ä¸Šçš„å¹³å‡å€¼
        maxout, _ = torch.max(x, dim=1, keepdim=True)  # max_out = [b, 1, 56, 56]  æ±‚xçš„æ¯ä¸ªåƒç´ åœ¨æ‰€æœ‰channelç›¸åŒä½ç½®ä¸Šçš„æœ€å¤§å€¼
        out = torch.cat([avgout, maxout], dim=1)  # x = [b, 2, 56, 56]  concatæ“ä½œ
        out = self.sigmoid(self.conv2d(out))  # x = [b, 1, 56, 56]  å·ç§¯æ“ä½œï¼Œèåˆavgå’Œmaxçš„ä¿¡æ¯ï¼Œå…¨æ–¹é¢è€ƒè™‘
        return out

class CBAM(nn.Module):
    def __init__(self, c1, c2):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttentionModule(c1)
        self.spatial_attention = SpatialAttentionModule()

    def forward(self, x):
        out = self.channel_attention(x) * x
        out = self.spatial_attention(out) * out
        return out

class TransformerLayer(nn.Module):
    """
         è¿™éƒ¨åˆ†ç›¸å½“äºåŸè®ºæ–‡ä¸­çš„å•ä¸ªEncoderéƒ¨åˆ†(åªç§»é™¤äº†ä¸¤ä¸ªNorméƒ¨åˆ†, å…¶ä»–ç»“æ„å’ŒåŸæ–‡ä¸­çš„Encodingä¸€æ¨¡ä¸€æ ·)
        """
    def __init__(self, c, num_heads):
        super().__init__()
 
        self.ln1 = nn.LayerNorm(c)
        self.q = nn.Linear(c, c, bias=False)
        self.k = nn.Linear(c, c, bias=False)
        self.v = nn.Linear(c, c, bias=False)
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)
        self.ln2 = nn.LayerNorm(c)
        self.fc1 = nn.Linear(c, 4*c, bias=False)
        self.fc2 = nn.Linear(4*c, c, bias=False)
        self.dropout = nn.Dropout(0.1)
        self.act = nn.ReLU(True)
 
    def forward(self, x):
        x_ = self.ln1(x)
        x = self.dropout(self.ma(self.q(x_), self.k(x_), self.v(x_))[0]) + x #æ®‹å·®
        x_ = self.ln2(x)#å½’ä¸€åŒ–
        x_ = self.fc2(self.dropout(self.act(self.fc1(x_))))
        x = x + self.dropout(x_)
        return x

class TransformerBlock(nn.Module):
    # Vision Transformer https://arxiv.org/abs/2010.11929
    # è¿™éƒ¨åˆ†ç›¸å½“äºåŸè®ºæ–‡ä¸­çš„Encoderséƒ¨åˆ† åªæ›¿æ¢äº†ä¸€äº›ç¼–ç æ–¹å¼å’Œæœ€åEncoderså‡ºæ¥æ•°æ®å¤„ç†æ–¹å¼
    def __init__(self, c1, c2, num_heads, num_layers):
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        self.c2 = c2

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2).unsqueeze(0).transpose(0, 3).squeeze(3)
        return self.tr(p + self.linear(p)).unsqueeze(3).transpose(0, 3).reshape(b, self.c2, w, h)

class Contract(nn.Module):
    # Contractå‡½æ•°æ”¹å˜è¾“å…¥ç‰¹å¾çš„shapeï¼Œå°†feature mapçš„wå’Œhç»´åº¦(ç¼©å°)çš„æ•°æ®æ”¶ç¼©åˆ°channelç»´åº¦ä¸Š(æ”¾å¤§)ã€‚å¦‚ï¼šx(1,64,80,80) to x(1,256,40,40)ã€‚
    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'
        s = self.gain
        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)
        return x.view(b, c * s * s, h // s, w // s)  # x(1,256,40,40)

class Expand(nn.Module):
    # ä¼šç”¨åœ¨yolo.pyçš„parse_modelæ¨¡å—ï¼ˆç”¨çš„ä¸å¤šï¼‰
    # Expandå‡½æ•°ä¹Ÿæ˜¯æ”¹å˜è¾“å…¥ç‰¹å¾çš„shapeï¼Œä¸è¿‡ä¸Contractçš„ç›¸åï¼Œ æ˜¯å°†channelç»´åº¦(å˜å°)çš„æ•°æ®æ‰©å±•åˆ°Wå’ŒHç»´åº¦(å˜å¤§)ã€‚å¦‚ï¼šx(1,64,80,80) to x(1,16,160,160)ã€‚
    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'
        s = self.gain
        x = x.view(b, s, s, c // s ** 2, h, w)  # x(1,2,2,16,80,80)
        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)
        return x.view(b, c // s ** 2, h * s, w * s)  # x(1,16,160,160)

def drop_path_f(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output

class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path_f(x, self.drop_prob, self.training)

def window_partition(x, window_size: int):
    """
    å°†feature mapæŒ‰ç…§window_sizeåˆ’åˆ†æˆä¸€ä¸ªä¸ªæ²¡æœ‰é‡å çš„window
    Args:
        x: (B, H, W, C)
        window_size (int): window size(M)

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    # permute: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H//Mh, W//Mh, Mw, Mw, C]
    # view: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B*num_windows, Mh, Mw, C]
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows

def window_reverse(windows, window_size: int, H: int, W: int):
    """
    å°†ä¸€ä¸ªä¸ªwindowè¿˜åŸæˆä¸€ä¸ªfeature map
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size(M)
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    # view: [B*num_windows, Mh, Mw, C] -> [B, H//Mh, W//Mw, Mh, Mw, C]
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    # permute: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B, H//Mh, Mh, W//Mw, Mw, C]
    # view: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H, W, C]
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x

class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # [Mh, Mw]
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # [2*Mh-1 * 2*Mw-1, nH]

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing="ij"))  # [2, Mh, Mw]
        coords_flatten = torch.flatten(coords, 1)  # [2, Mh*Mw]
        # [2, Mh*Mw, 1] - [2, 1, Mh*Mw]
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # [2, Mh*Mw, Mh*Mw]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # [Mh*Mw, Mh*Mw, 2]
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # [Mh*Mw, Mh*Mw]
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask: Optional[torch.Tensor] = None):
        """
        Args:
            x: input features with shape of (num_windows*B, Mh*Mw, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        # [batch_size*num_windows, Mh*Mw, total_embed_dim]
        B_, N, C = x.shape
        # qkv(): -> [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]
        # reshape: -> [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]
        # permute: -> [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]
        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)

        # transpose: -> [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]
        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        # relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -> [Mh*Mw,Mh*Mw,nH]
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # [nH, Mh*Mw, Mh*Mw]
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            # mask: [nW, Mh*Mw, Mh*Mw]
            nW = mask.shape[0]  # num_windows
            # attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]
            # mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        # @: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]
        # transpose: -> [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]
        # reshape: -> [batch_size*num_windows, Mh*Mw, total_embed_dim]
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class SwinTransformerLayer(nn.Module):
    # Vision Transformer https://arxiv.org/abs/2010.11929
    def __init__(self, c, num_heads, window_size=7, shift_size=0, 
                mlp_ratio = 4, qkv_bias=False, drop=0., attn_drop=0., drop_path=0.,
                act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        if num_heads > 10:
            drop_path = 0.1
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio

        self.norm1 = norm_layer(c)
        self.attn = WindowAttention(
            c, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,
            attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(c)
        mlp_hidden_dim = int(c * mlp_ratio)
        self.mlp = Mlp(in_features=c, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
        
    def create_mask(self, x, H, W):
        # calculate attention mask for SW-MSA
        # ä¿è¯Hpå’ŒWpæ˜¯window_sizeçš„æ•´æ•°å€
        Hp = int(np.ceil(H / self.window_size)) * self.window_size
        Wp = int(np.ceil(W / self.window_size)) * self.window_size
        # æ‹¥æœ‰å’Œfeature mapä¸€æ ·çš„é€šé“æ’åˆ—é¡ºåºï¼Œæ–¹ä¾¿åç»­window_partition
        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # [1, Hp, Wp, 1]
        h_slices = ( (0, -self.window_size),
                    slice(-self.window_size, -self.shift_size),
                    slice(-self.shift_size, None))
        w_slices = (slice(0, -self.window_size),
                    slice(-self.window_size, -self.shift_size),
                    slice(-self.shift_size, None))
        cnt = 0
        for h in h_slices:
            for w in w_slices:
                img_mask[:, h, w, :] = cnt
                cnt += 1

        mask_windows = window_partition(img_mask, self.window_size)  # [nW, Mh, Mw, 1]
        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)  # [nW, Mh*Mw]
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]
        # [nW, Mh*Mw, Mh*Mw]
        attn_mask = attn_mask.masked_fill(attn_mask != 0, torch.tensor(-100.0)).masked_fill(attn_mask == 0, torch.tensor(0.0))
        return attn_mask

    def forward(self, x):
        b, c, w, h = x.shape
        x = x.permute(0, 3, 2, 1).contiguous() # [b,h,w,c]

        attn_mask = self.create_mask(x, h, w) # [nW, Mh*Mw, Mh*Mw]
        shortcut = x
        x = self.norm1(x)
        
        pad_l = pad_t = 0
        pad_r = (self.window_size - w % self.window_size) % self.window_size
        pad_b = (self.window_size - h % self.window_size) % self.window_size
        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
        _, hp, wp, _ = x.shape

        if self.shift_size > 0:
            # print(f"shift size: {self.shift_size}")
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x
            attn_mask = None
        
        x_windows = window_partition(shifted_x, self.window_size) # [nW*B, Mh, Mw, C]
        x_windows = x_windows.view(-1, self.window_size * self.window_size, c) # [nW*B, Mh*Mw, C]

        attn_windows = self.attn(x_windows, mask=attn_mask)  # [nW*B, Mh*Mw, C]

        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, c)  # [nW*B, Mh, Mw, C]
        shifted_x = window_reverse(attn_windows, self.window_size, hp, wp)  # [B, H', W', C]
        
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        
        if pad_r > 0 or pad_b > 0:
            # æŠŠå‰é¢padçš„æ•°æ®ç§»é™¤æ‰
            x = x[:, :h, :w, :].contiguous()

        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        
        x = x.permute(0, 3, 2, 1).contiguous()
        return x # (b, self.c2, w, h)

class SwinTransformerBlock(nn.Module):
    def __init__(self, c1, c2, num_heads, num_layers, window_size=8):
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)

        self.window_size = window_size
        self.shift_size = window_size // 2
        self.tr = nn.Sequential(*(SwinTransformerLayer(c2, num_heads=num_heads, window_size=window_size,  shift_size=0 if (i % 2 == 0) else self.shift_size ) for i in range(num_layers)))

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        x = self.tr(x)
        return x

class Concat(nn.Module):
    # è¿™ä¸ªå‡½æ•°æ˜¯è®²è‡ªèº«ï¼ˆa list of tensorsï¼‰æŒ‰ç…§æŸä¸ªç»´åº¦è¿›è¡Œconcatï¼Œå¸¸ç”¨æ¥åˆå¹¶å‰åä¸¤ä¸ªfeature mapï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢yolov5sç»“æ„å›¾ä¸­çš„Concatã€‚
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)

class GhostConv(nn.Module):
    """2020æå‡ºçš„æ–°å‹çš„è½»é‡åŒ–ç½‘ç»œæ¨¡å—Ghostï¼Œè¯¥æ¨¡å—ä¸èƒ½å¢åŠ MAPï¼Œä½†æ˜¯å¯ä»¥å¤§å¤§å‡å°‘æ¨¡å‹çš„è®¡ç®—é‡
    å¯ä»¥ç”¨GhostConvä»£æ›¿ä¸€èˆ¬çš„Convï¼ŒGhostBottleneckä»£æ›¿C3ï¼Œè‡³äºåœ¨å“ªäº›ä½ç½®ä»£æ›¿ï¼Œå¯ä»¥è‡ªå·±å†³å®šã€‚
    Ghost Convolution https://github.com/huawei-noah/ghostnet
    Ghost Convolution å¹»è±¡å·ç§¯  è½»é‡åŒ–ç½‘ç»œå·ç§¯æ¨¡å—
    è®ºæ–‡: https://arxiv.org/abs/1911.11907
    æºç : https://github.com/huawei-noah/ghostnet
    """
    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups
        super().__init__()
        c_ = c2 // 2  # hidden channels
        # ç¬¬ä¸€æ­¥å·ç§¯: å°‘é‡å·ç§¯, ä¸€èˆ¬æ˜¯ä¸€åŠçš„è®¡ç®—é‡
        self.cv1 = Conv(c1, c_, k, s, None, g, act)
        # ç¬¬äºŒæ­¥å·ç§¯: cheap operations ä½¿ç”¨3x3æˆ–5x5çš„å·ç§¯, å¹¶ä¸”æ˜¯é€ä¸ªç‰¹å¾å›¾çš„è¿›è¡Œå·ç§¯ï¼ˆDepth-wise convolutionalï¼‰
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act)

    def forward(self, x):
        y = self.cv1(x)
        return torch.cat([y, self.cv2(y)], 1)

class GhostBottleneck(nn.Module):
    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet
    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride
        super().__init__()
        c_ = c2 // 2
        self.conv = nn.Sequential(GhostConv(c1, c_, 1, 1),  # pw
                                  DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw
                                  GhostConv(c_, c2, 1, 1, act=False))  # pw-linear
        # æ³¨æ„, æºç ä¸­å¹¶ä¸æ˜¯ç›´æ¥Identityè¿æ¥, è€Œæ˜¯å…ˆç»è¿‡ä¸€ä¸ªDWConv + Conv, å†è¿›è¡Œshortcutè¿æ¥çš„ã€‚
        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False),
                                      Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()

    def forward(self, x):
        return self.conv(x) + self.shortcut(x)

class AutoShape(nn.Module):
    #è¿™ä¸ªæ¨¡å—æ˜¯ä¸€ä¸ªæ¨¡å‹æ‰©å±•æ¨¡å—ï¼Œç»™æ¨¡å‹å°è£…æˆåŒ…å«å‰å¤„ç†ã€æ¨ç†ã€åå¤„ç†çš„æ¨¡å—(é¢„å¤„ç† + æ¨ç† + nms)ï¼Œç”¨çš„ä¸å¤šã€‚
    """åœ¨yolo.pyä¸­Modelç±»çš„autoshapeå‡½æ•°ä¸­ä½¿ç”¨
        å°†modelå°è£…æˆåŒ…å«å‰å¤„ç†ã€æ¨ç†ã€åå¤„ç†çš„æ¨¡å—(é¢„å¤„ç† + æ¨ç† + nms)  ä¹Ÿæ˜¯ä¸€ä¸ªæ‰©å±•æ¨¡å‹åŠŸèƒ½çš„æ¨¡å—
        autoshapeæ¨¡å—åœ¨trainä¸­ä¸ä¼šè¢«è°ƒç”¨ï¼Œå½“æ¨¡å‹è®­ç»ƒç»“æŸåï¼Œä¼šé€šè¿‡è¿™ä¸ªæ¨¡å—å¯¹å›¾ç‰‡è¿›è¡Œé‡å¡‘ï¼Œæ¥æ–¹ä¾¿æ¨¡å‹çš„é¢„æµ‹
        è‡ªåŠ¨è°ƒæ•´shapeï¼Œæˆ‘ä»¬è¾“å…¥çš„å›¾åƒå¯èƒ½ä¸ä¸€æ ·ï¼Œå¯èƒ½æ¥è‡ªcv2/np/PIL/torch å¯¹è¾“å…¥è¿›è¡Œé¢„å¤„ç† è°ƒæ•´å…¶shapeï¼Œ
        è°ƒæ•´shapeåœ¨datasets.pyæ–‡ä»¶ä¸­,è¿™ä¸ªå®åœ¨é¢„æµ‹é˜¶æ®µä½¿ç”¨çš„,model.eval(),æ¨¡å‹å°±å·²ç»æ— æ³•è®­ç»ƒè¿›å…¥é¢„æµ‹æ¨¡å¼äº†
        input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
        """
    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
    conf = 0.25  # NMS confidence threshold
    iou = 0.45  # NMS IoU threshold
    classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs
    multi_label = False  # NMS multiple labels per box
    max_det = 1000  # maximum number of detections per image

    def __init__(self, model):
        super().__init__()
        #å¼€å¯éªŒè¯æ¨¡å¼
        self.model = model.eval()

    def autoshape(self):
        LOGGER.info('AutoShape already enabled, skipping... ')  # model already converted to model.autoshape()
        return self

    def _apply(self, fn):
        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers
        self = super()._apply(fn)
        m = self.model.model[-1]  # Detect()
        m.stride = fn(m.stride)
        m.grid = list(map(fn, m.grid))
        if isinstance(m.anchor_grid, list):
            m.anchor_grid = list(map(fn, m.anchor_grid))
        return self

    @torch.no_grad()
    def forward(self, imgs, size=640, augment=False, profile=False):
        # è¿™é‡Œçš„imgsé’ˆå¯¹ä¸åŒçš„æ–¹æ³•è¯»å…¥ï¼Œå®˜æ–¹ä¹Ÿç»™äº†å…·ä½“çš„æ–¹æ³•ï¼Œsizeæ˜¯å›¾ç‰‡çš„å°ºå¯¸ï¼Œå°±æ¯”å¦‚æœ€ä¸Šé¢å›¾ç‰‡é‡Œé¢çš„è¾“å…¥608*608*3
        # Inference from various sources. For height=640, width=1280, RGB images example inputs are:
        #   file:       imgs = 'data/images/zidane.jpg'  # str or PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images

        t = [time_sync()]
        p = next(self.model.parameters())  # for device and type
        # å›¾ç‰‡å¦‚æœæ˜¯tensoræ ¼å¼ è¯´æ˜æ˜¯é¢„å¤„ç†è¿‡çš„, ç›´æ¥æ­£å¸¸è¿›è¡Œå‰å‘æ¨ç†å³å¯ nmsåœ¨æ¨ç†ç»“æŸè¿›è¡Œ(å‡½æ•°å¤–å†™)
        if isinstance(imgs, torch.Tensor):  # torch
            with amp.autocast(enabled=p.device.type != 'cpu'):
                return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference

        # Pre-process å›¾ç‰‡ä¸æ˜¯tensoræ ¼å¼ å°±å…ˆå¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†  Pre-process
        n, imgs = (len(imgs), imgs) if isinstance(imgs, list) else (1, [imgs])  # number of images, list of images
        shape0, shape1, files = [], [], []  # image and inference shapes, filenames
        for i, im in enumerate(imgs):
            f = f'image{i}'  # filename
            if isinstance(im, (str, Path)):  # filename or uri
                im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im
                im = np.asarray(exif_transpose(im))
            elif isinstance(im, Image.Image):  # PIL Image
                im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f
            files.append(Path(f).with_suffix('.jpg').name)
            if im.shape[0] < 5:  # image in CHW
                im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)
            im = im[..., :3] if im.ndim == 3 else np.tile(im[..., None], 3)  # enforce 3ch input
            s = im.shape[:2]  # HWC
            shape0.append(s)  # image shape
            g = (size / max(s))  # gain
            shape1.append([y * g for y in s])
            imgs[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update
        shape1 = [make_divisible(x, int(self.stride.max())) for x in np.stack(shape1, 0).max(0)]  # inference shape
        x = [letterbox(im, new_shape=shape1, auto=False)[0] for im in imgs]  # pad
        x = np.stack(x, 0) if n > 1 else x[0][None]  # stack
        x = np.ascontiguousarray(x.transpose((0, 3, 1, 2)))  # BHWC to BCHW
        x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32
        t.append(time_sync())

        with amp.autocast(enabled=p.device.type != 'cpu'):
            # Inference é¢„å¤„ç†ç»“æŸå†è¿›è¡Œå‰å‘æ¨ç†  Inference
            y = self.model(x, augment, profile)[0]  # forward å‰å‘æ¨ç†
            t.append(time_sync())

            # Post-process å‰å‘æ¨ç†ç»“æŸå è¿›è¡Œåå¤„ç†Post-process  nms
            y = non_max_suppression(y, self.conf, iou_thres=self.iou, classes=self.classes,
                                    multi_label=self.multi_label, max_det=self.max_det)  # NMS
            for i in range(n):
                scale_coords(shape1, y[i][:, :4], shape0[i])# å°†nmsåçš„é¢„æµ‹ç»“æœæ˜ å°„å›åŸå›¾å°ºå¯¸

            t.append(time_sync())
            return Detections(imgs, y, files, t, self.names, x.shape)
#ç”¨çš„æå°‘
class Detections:
    # YOLOv5 detections class for inference results
    def __init__(self, imgs, pred, files, times=None, names=None, shape=None):
        super().__init__()
        d = pred[0].device  # device
        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # normalizations
        self.imgs = imgs  # list of images as numpy arrays
        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)
        self.names = names  # class names
        self.files = files  # image filenames
        self.xyxy = pred  # xyxy pixels
        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels
        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized
        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized
        self.n = len(self.pred)  # number of images (batch size)
        self.t = tuple((times[i + 1] - times[i]) * 1000 / self.n for i in range(3))  # timestamps (ms)
        self.s = shape  # inference BCHW shape

    def display(self, pprint=False, show=False, save=False, crop=False, render=False, save_dir=Path('')):
        crops = []
        for i, (im, pred) in enumerate(zip(self.imgs, self.pred)):
            s = f'image {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # string
            if pred.shape[0]:
                for c in pred[:, -1].unique():
                    n = (pred[:, -1] == c).sum()  # detections per class
                    s += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "  # add to string
                if show or save or render or crop:
                    annotator = Annotator(im, example=str(self.names))
                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class
                        label = f'{self.names[int(cls)]} {conf:.2f}'
                        if crop:
                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None
                            crops.append({'box': box, 'conf': conf, 'cls': cls, 'label': label,
                                          'im': save_one_box(box, im, file=file, save=save)})
                        else:  # all others
                            annotator.box_label(box, label, color=colors(cls))
                    im = annotator.im
            else:
                s += '(no detections)'

            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np
            if pprint:
                LOGGER.info(s.rstrip(', '))
            if show:
                im.show(self.files[i])  # show
            if save:
                f = self.files[i]
                im.save(save_dir / f)  # save
                if i == self.n - 1:
                    LOGGER.info(f"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}")
            if render:
                self.imgs[i] = np.asarray(im)
        if crop:
            if save:
                LOGGER.info(f'Saved results to {save_dir}\n')
            return crops

    def print(self):
        self.display(pprint=True)  # print results
        LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' %
                    self.t)

    def show(self):
        self.display(show=True)  # show results

    def save(self, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True)  # increment save_dir
        self.display(save=True, save_dir=save_dir)  # save results

    def crop(self, save=True, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True) if save else None
        return self.display(crop=True, save=save, save_dir=save_dir)  # crop results

    def render(self):
        self.display(render=True)  # render results
        return self.imgs

    def pandas(self):
        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])
        new = copy(self)  # return copy
        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns
        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns
        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):
            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update
            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])
        return new

    def tolist(self):
        # return a list of Detections objects, i.e. 'for result in results.tolist():'
        x = [Detections([self.imgs[i]], [self.pred[i]], self.names, self.s) for i in range(self.n)]
        for d in x:
            for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:
                setattr(d, k, getattr(d, k)[0])  # pop out of list
        return x

    def __len__(self):
        return self.n

class Classify(nn.Module):
    # Classification head, i.e. x(b,c1,20,20) to x(b,c2)
    """
            è¿™æ˜¯ä¸€ä¸ªäºŒçº§åˆ†ç±»æ¨¡å—, æ¯”å¦‚åšè½¦ç‰Œçš„è¯†åˆ«, å…ˆè¯†åˆ«å‡ºè½¦ç‰Œ, å¦‚æœæƒ³å¯¹è½¦ç‰Œä¸Šçš„å­—è¿›è¡Œè¯†åˆ«, å°±éœ€è¦äºŒçº§åˆ†ç±»è¿›ä¸€æ­¥æ£€æµ‹.
            å¦‚æœå¯¹æ¨¡å‹è¾“å‡ºçš„åˆ†ç±»å†è¿›è¡Œåˆ†ç±», å°±å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å—. ä¸è¿‡è¿™é‡Œè¿™ä¸ªç±»å†™çš„æ¯”è¾ƒç®€å•, è‹¥è¿›è¡Œå¤æ‚çš„äºŒçº§åˆ†ç±», å¯ä»¥æ ¹æ®è‡ªå·±çš„å®é™…ä»»åŠ¡å¯ä»¥æ”¹å†™, è¿™é‡Œä»£ç ä¸å”¯ä¸€.
            Classification head, i.e. x(b,c1,20,20) to x(b,c2)
            ç”¨äºç¬¬äºŒçº§åˆ†ç±»   å¯ä»¥æ ¹æ®è‡ªå·±çš„ä»»åŠ¡è‡ªå·±æ”¹å†™ï¼Œæ¯”è¾ƒç®€å•
            """
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        self.aap = nn.AdaptiveAvgPool2d(1)  # to x(b,c1,1,1) è‡ªé€‚åº”å¹³å‡æ± åŒ–æ“ä½œ
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g)  # to x(b,c2,1,1)
        self.flat = nn.Flatten()# å±•å¹³

    def forward(self, x):
        # å…ˆè‡ªé€‚åº”å¹³å‡æ± åŒ–æ“ä½œï¼Œç„¶åæ‹¼æ¥
        z = torch.cat([self.aap(y) for y in (x if isinstance(x, list) else [x])], 1)  # cat if list
        return self.flat(self.conv(z))  # flatten to x(b,c2)# å¯¹zè¿›è¡Œå±•å¹³æ“ä½œ

#---------------------------------------æ–°è‡ªé€‚åº”èåˆæ¨¡å—----------------------------------------
class AdaptADD(nn.Module):
    # è¿™ä¸ªå‡½æ•°æ˜¯è®²è‡ªèº«ï¼ˆa list of tensorsï¼‰æŒ‰ç…§æŸä¸ªç»´åº¦è¿›è¡Œconcatï¼Œå¸¸ç”¨æ¥åˆå¹¶å‰åä¸¤ä¸ªfeature mapï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢yolov5sç»“æ„å›¾ä¸­çš„Concatã€‚
    # Concatenate a list of tensors along dimension
    def __init__(self, level, out_ch, dimension, dim1, dim2, dim3=1, rfb=False):
        super().__init__()
        self.level= level
        self.d = dimension
        self.dims = [dim1, dim2, dim3]
        compress_c = 8 if rfb else 16
        self.compress_level = add_conv(self.dims[2], self.dims[0], 1, 1)#ç¬¬ä¸‰å±‚ç‰¹å¾é™ç»´
        self.weight_map = add_conv(self.dims[0], compress_c, 1, 1)# è®¡ç®—æƒé‡
        self.weight_levels = nn.Conv2d(compress_c * level, level, kernel_size=1, stride=1, padding=0)
        self.expand = add_conv(self.dims[0], out_ch, 3, 1)

    def forward(self, x):
        weights = []
        weight0 = self.weight_map(x[0])
        weight1 = self.weight_map(x[1])
        weights.append(weight0)
        weights.append(weight1)

        if self.level == 3:
            map = self.compress_level(x[2])
            weight2 = self.weight_map(map)
            weights.append(weight2)

        weights = torch.cat(weights, self.d) # å°†å„ä¸ªæƒé‡çŸ©é˜µæŒ‰ç…§é€šé“concat
        #print (weights.shape)
        weights = self.weight_levels(weights)
        levels_weight = F.softmax(weights, dim=1)

        if self.level == 2:
            fused_out_reduced = x[0] * levels_weight[:, 0:1, :, :] + x[1] * levels_weight[:, 1:, :, :]
        else:
            fused_out_reduced = x[0] * levels_weight[:, 0:1, :, :] + x[1] * levels_weight[:, 1:2, :, :] + map * levels_weight[:, 2:, :, :]

        fused_out_reduced = self.expand(fused_out_reduced)

        return  fused_out_reduced

class AdaptConcat(nn.Module):
    # è¿™ä¸ªå‡½æ•°æ˜¯è®²è‡ªèº«ï¼ˆa list of tensorsï¼‰æŒ‰ç…§æŸä¸ªç»´åº¦è¿›è¡Œconcatï¼Œå¸¸ç”¨æ¥åˆå¹¶å‰åä¸¤ä¸ªfeature mapï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢yolov5sç»“æ„å›¾ä¸­çš„Concatã€‚
    # Concatenate a list of tensors along dimension
    def __init__(self, level, dimension, dim1, dim2, dim3=1, rfb=False):
        super().__init__()
        self.level= level
        self.d = dimension
        self.dims = [dim1, dim2, dim3]
        compress_c = 8 if rfb else 16
        #self.compress_level = add_conv(self.dims[2], self.dims[0], 1, 1)#ç¬¬ä¸‰å±‚ç‰¹å¾é™ç»´
        self.weight_map0 = add_conv(self.dims[0], compress_c, 1, 1)# è®¡ç®—æƒé‡
        self.weight_map1 = add_conv(self.dims[1], compress_c, 1, 1)  # è®¡ç®—æƒé‡
        self.weight_map2 = add_conv(self.dims[2], compress_c, 1, 1)  # è®¡ç®—æƒé‡
        self.weight_levels = nn.Conv2d(compress_c * level, level, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        weights = []
        weight0 = self.weight_map0(x[0])
        weight1 = self.weight_map1(x[1])
        weights.append(weight0)
        weights.append(weight1)

        if self.level == 3:
            weight2 = self.weight_map2(x[2])
            weights.append(weight2)

        weights = torch.cat(weights, self.d) # å°†å„ä¸ªæƒé‡çŸ©é˜µæŒ‰ç…§é€šé“concat
        #print (weights.shape)
        weights = self.weight_levels(weights)
        levels_weight = F.softmax(weights, dim=1)

        x0 = x[0] * levels_weight[:, 0:1, :, :]
        x1 = x[1] * levels_weight[:, 1:2, :, :]
        fused_out_reduced = torch.cat((x0, x1), 1)

        if self.level == 3:
            x2 = x[2] * levels_weight[:, 2:, :, :]
            fused_out_reduced = torch.cat((x0, x1, x2), 1)

        return  fused_out_reduced

class AdConcat2(nn.Module):
    # ç»“åˆBiFPN è®¾ç½®å¯å­¦ä¹ å‚æ•° å­¦ä¹ ä¸åŒåˆ†æ”¯çš„æƒé‡
    # ä¸¤ä¸ªåˆ†æ”¯concatæ“ä½œ
    def __init__(self, dimension=1):
        super(AdConcat2, self).__init__()
        self.d = dimension
        self.w = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # å°†æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # Fast normalized fusion
        x = [weight[0] * x[0], weight[1] * x[1]]
        return torch.cat(x, self.d)

class AdConcat3(nn.Module):
    # ä¸‰ä¸ªåˆ†æ”¯concatæ“ä½œ
    def __init__(self, dimension=1):
        super(AdConcat3, self).__init__()
        self.d = dimension
        # è®¾ç½®å¯å­¦ä¹ å‚æ•° nn.Parameterçš„ä½œç”¨æ˜¯ï¼šå°†ä¸€ä¸ªä¸å¯è®­ç»ƒçš„ç±»å‹Tensorè½¬æ¢æˆå¯ä»¥è®­ç»ƒçš„ç±»å‹parameter
        # å¹¶ä¸”ä¼šå‘å®¿ä¸»æ¨¡å‹æ³¨å†Œè¯¥å‚æ•° æˆä¸ºå…¶ä¸€éƒ¨åˆ† å³model.parameters()ä¼šåŒ…å«è¿™ä¸ªparameter
        # ä»è€Œåœ¨å‚æ•°ä¼˜åŒ–çš„æ—¶å€™å¯ä»¥è‡ªåŠ¨ä¸€èµ·ä¼˜åŒ–
        self.w = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # å°†æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # Fast normalized fusion
        x = [weight[0] * x[0], weight[1] * x[1], weight[2] * x[2]]
        return torch.cat(x, self.d)

class Adapt_Add2(nn.Module):
    # ç»“åˆBiFPN è®¾ç½®å¯å­¦ä¹ å‚æ•° å­¦ä¹ ä¸åŒåˆ†æ”¯çš„æƒé‡
    # ä¸¤ä¸ªåˆ†æ”¯addæ“ä½œ
    def __init__(self):
        super(Adapt_Add2, self).__init__()
        # è®¾ç½®å¯å­¦ä¹ å‚æ•° nn.Parameterçš„ä½œç”¨æ˜¯ï¼šå°†ä¸€ä¸ªä¸å¯è®­ç»ƒçš„ç±»å‹Tensorè½¬æ¢æˆå¯ä»¥è®­ç»ƒçš„ç±»å‹parameter
        # å¹¶ä¸”ä¼šå‘å®¿ä¸»æ¨¡å‹æ³¨å†Œè¯¥å‚æ•° æˆä¸ºå…¶ä¸€éƒ¨åˆ† å³model.parameters()ä¼šåŒ…å«è¿™ä¸ªparameter
        # ä»è€Œåœ¨å‚æ•°ä¼˜åŒ–çš„æ—¶å€™å¯ä»¥è‡ªåŠ¨ä¸€èµ·ä¼˜åŒ–
        self.w = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001
        self.silu = nn.SiLU()

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)

        return self.silu(weight[0] * x[0] + weight[1] * x[1])

class Adapt_Add3(nn.Module):
    # ä¸‰ä¸ªåˆ†æ”¯addæ“ä½œ
    def __init__(self, d1, d2, d3):
        super(Adapt_Add3, self).__init__()

        self.w = nn.Parameter(torch.ones(3, dtype=torch.float32), requires_grad=True)
        self.epsilon = 0.0001

        self.conv = nn.Conv2d(d1, d3, kernel_size=1, stride=1, padding=0)
        self.silu = nn.SiLU()

    def forward(self, x):
        w = self.w
        weight = w / (torch.sum(w, dim=0) + self.epsilon)  # å°†æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # Fast normalized fusion
        return self.silu(weight[0] * self.conv(x[0]) + weight[1] * self.conv(x[1]) + weight[2] * x[2])

def add_conv(in_ch, out_ch, ksize=1, stride=1):
    """
    Add a conv2d / batchnorm / leaky ReLU block.
    Args:
        in_ch (int): number of input channels of the convolution layer.
        out_ch (int): number of output channels of the convolution layer.
        ksize (int): kernel size of the convolution layer.
        stride (int): stride of the convolution layer.
    Returns:
        stage (Sequential) : Sequential layers composing a convolution block.
    """
    stage = nn.Sequential()
    pad = (ksize - 1) // 2
    stage.add_module('conv', nn.Conv2d(in_channels=in_ch,
                                       out_channels=out_ch, kernel_size=ksize, stride=stride,
                                       padding=pad, bias=False))
    stage.add_module('batch_norm', nn.BatchNorm2d(out_ch))
    stage.add_module('leaky', nn.LeakyReLU(0.1))
    return stage

class ASFF(nn.Module):
    def __init__(self, level, rfb=False, vis=False):
        super(ASFF, self).__init__()
        self.level = level
        # è¾“å…¥çš„ä¸‰ä¸ªç‰¹å¾å±‚çš„channels, æ ¹æ®å®é™…ä¿®æ”¹ï¼Œ è°ƒæ•´åˆ°ä¸€æ ·çš„å°ºåº¦
        self.dim = [512, 256, 256]# è¾“å…¥çš„ä¸‰å±‚ç‰¹å¾ï¼Œé€šé“æ•°åˆ†åˆ«ä¸º512ï¼Œ256ï¼Œ256
        self.inter_dim = self.dim[self.level] # ä¸­é—´è¾“å‡ºé€šé“æ•°ï¼Œå¾—åˆ°ç¬¬å‡ å±‚çš„èåˆç»“æœï¼Œè¾“å‡ºé€šé“æ•°å°±æ˜¯è¯¥å±‚çš„é€šé“æ•°
        # æ¯ä¸ªå±‚çº§ä¸‰è€…è¾“å‡ºé€šé“æ•°éœ€è¦ä¸€è‡´
        if level==0: #ç¬¬0å±‚éœ€è¦å…¶ä»–ä¸¤å±‚ç»è¿‡ä¸‹é‡‡æ · è¾“å…¥é€šé“åˆ†åˆ«ä¸ºè‡ªèº«é€šé“æ•°ï¼Œè¾“å‡ºé€šé“ä¸ºç¬¬0å±‚çš„é€šé“æ•°ï¼Œ å·ç§¯æ ¸å¤§å°ä¸º3ï¼Œ æ­¥é•¿ä¸º2
            self.stride_level_1 = add_conv(self.dim[1], self.inter_dim, 3, 2)
            self.stride_level_2 = add_conv(self.dim[2], self.inter_dim, 3, 2)
            self.expand = add_conv(self.inter_dim, 1024, 3, 1)
        elif level==1:
            self.compress_level_0 = add_conv(self.dim[0], self.inter_dim, 1, 1)
            self.stride_level_2 = add_conv(self.dim[2], self.inter_dim, 3, 2)
            self.expand = add_conv(self.inter_dim, 512, 3, 1)
        elif level==2:
            self.compress_level_0 = add_conv(self.dim[0], self.inter_dim, 1, 1)
            if self.dim[1] != self.dim[2]:
                self.compress_level_1 = add_conv(self.dim[1], self.inter_dim, 1, 1)
            self.expand = add_conv(self.inter_dim, 256, 3, 1)

        compress_c = 8 if rfb else 16  #when adding rfb, we use half number of channels to save memory

        self.weight_level_0 = add_conv(self.inter_dim, compress_c, 1, 1)
        self.weight_level_1 = add_conv(self.inter_dim, compress_c, 1, 1)
        self.weight_level_2 = add_conv(self.inter_dim, compress_c, 1, 1)

        self.weight_levels = nn.Conv2d(compress_c*3, 3, kernel_size=1, stride=1, padding=0)
        self.vis= vis

	# å°ºåº¦å¤§å° level_0 < level_1 < level_2
    def forward(self, x_level_0, x_level_1, x_level_2):
        if self.level==0:
            level_0_resized = x_level_0
            level_1_resized = self.stride_level_1(x_level_1)

            level_2_downsampled_inter =F.max_pool2d(x_level_2, 3, stride=2, padding=1)
            level_2_resized = self.stride_level_2(level_2_downsampled_inter)

        elif self.level==1:
            level_0_compressed = self.compress_level_0(x_level_0)
            level_0_resized =F.interpolate(level_0_compressed, scale_factor=2, mode='nearest')
            level_1_resized =x_level_1
            level_2_resized =self.stride_level_2(x_level_2)
        elif self.level==2:
            level_0_compressed = self.compress_level_0(x_level_0)
            level_0_resized =F.interpolate(level_0_compressed, scale_factor=4, mode='nearest')
            if self.dim[1] != self.dim[2]:
                level_1_compressed = self.compress_level_1(x_level_1)
                level_1_resized = F.interpolate(level_1_compressed, scale_factor=2, mode='nearest')
            else:
                level_1_resized =F.interpolate(x_level_1, scale_factor=2, mode='nearest')
            level_2_resized =x_level_2

        level_0_weight_v = self.weight_level_0(level_0_resized)
        level_1_weight_v = self.weight_level_1(level_1_resized)
        level_2_weight_v = self.weight_level_2(level_2_resized)
        levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v),1)
        levels_weight = self.weight_levels(levels_weight_v)
        levels_weight = F.softmax(levels_weight, dim=1)   # alphaç­‰äº§ç”Ÿ

        fused_out_reduced = level_0_resized * levels_weight[:,0:1,:,:]+\
                            level_1_resized * levels_weight[:,1:2,:,:]+\
                            level_2_resized * levels_weight[:,2:,:,:]

        out = self.expand(fused_out_reduced)

        if self.vis:
            return out, levels_weight, fused_out_reduced.sum(dim=1)
        else:
            return out

#--------------------------------------------åŠ å…¥æ³¨æ„åŠ›æ¨¡å—----------------------------------------

class CoorAttention(nn.Module):
    """
    CA Coordinate Attention ååŒæ³¨æ„åŠ›æœºåˆ¶
    è®ºæ–‡ CVPR2021: https://arxiv.org/abs/2103.02907
    æºç : https://github.com/Andrew-Qibin/CoordAttention/blob/main/coordatt.py
    CAæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ä¸ªSpatial Attention ç›¸æ¯”äºSAMçš„7x7å·ç§¯, CAå»ºç«‹äº†è¿œç¨‹ä¾èµ–
    """

    def __init__(self, c1, c2, reduction=32):
        super(CoorAttention, self).__init__()
        # [B, C, H, W] -> [B, C, H, 1]
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1)) # è‡ªé€‚åº”å¹³å‡æ± åŒ–ï¼ŒæŒ‡å®šè¾“å‡ºï¼ˆH,Wï¼‰ï¼Œxè½´å³ä¸ºæ°´å¹³æ–¹å‘wï¼Œè¿›è€Œä½¿wçš„å€¼å˜ä¸º1
        # [B, C, H, W] -> [B, C, 1, W]
        self.pool_w= nn.AdaptiveAvgPool2d((1, None)) #åœ¨yè½´è¿›è¡Œå¹³å‡æ± åŒ–æ“ä½œï¼Œyè½´ä¸ºå‚ç›´æ–¹å‘hï¼Œè¿›è€Œä½¿hçš„å€¼å˜ä¸º1

        c_ = max(8, c1 // reduction)   # å¯¹ä¸­é—´å±‚channelåšä¸€ä¸ªé™åˆ¶ ä¸å¾—å°‘äº8
        # å°†xè½´ä¿¡æ¯å’Œyè½´ä¿¡æ¯èåˆåœ¨ä¸€èµ·
        self.conv1 = nn.Conv2d(c1, c_, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(c_)
        self.act = nn.Hardswish()  # è¿™é‡Œè‡ªå·±å¯ä»¥å®éªŒä»€ä¹ˆæ¿€æ´»å‡½æ•°æœ€ä½³ è®ºæ–‡é‡Œæ˜¯hard-swish
        #self.act = nn.ReLU()

        self.conv_w = nn.Conv2d(c_, c2, kernel_size=1, stride=1, padding=0)
        self.conv_h = nn.Conv2d(c_, c2, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        identity = x
        n, c, h, w = x.size()
        # [B, C, H, W] -> [B, C, H, 1]
        x_h = self.pool_h(x)   # h avg pool
        # [B, C, H, W] -> [B, C, 1, W] -> [B, C, W, 1]
        x_w = self.pool_w(x).permute(0, 1, 3, 2)  # w avg pool

        y = torch.cat([x_h, x_w], dim=2)  # [B, C, H+W, 1]
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)

        # split  x_h: [B, C, H, 1]  x_w: [B, C, W, 1]
        x_h, x_w = torch.split(y, [h, w], dim=2)
        # [B, C, W, 1] -> [B, C, 1, W]
        x_w = x_w.permute(0, 1, 3, 2)

        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()

        # åŸºäºWå’ŒHæ–¹å‘åšæ³¨æ„åŠ›æœºåˆ¶ å»ºç«‹è¿œç¨‹ä¾èµ–å…³ç³»
        out = identity * a_w * a_h

        return out

class CABottleneck(nn.Module):
    """
           Standard bottleneck  Conv+Conv+shortcut
            :params c1: ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å…¥channel
            :params c2: ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å‡ºchannel
            :params shortcut: bool æ˜¯å¦æœ‰shortcutè¿æ¥ é»˜è®¤æ˜¯True
            :params g: å·ç§¯åˆ†ç»„çš„ä¸ªæ•°  =1å°±æ˜¯æ™®é€šå·ç§¯  >1å°±æ˜¯æ·±åº¦å¯åˆ†ç¦»å·ç§¯
            :params e: expansion ratio  e*c2å°±æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºchannel=ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥channel
            """
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5, reduction=32):  # ch_in, ch_out, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        self.ca = CoorAttention(c2, c2, reduction)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.ca(self.cv2(self.cv1(x))) if self.add else self.ca(self.cv2(self.cv1(x)))

class C3CA(C3):
    # C3 module with CABottleneck()
    # å°†CAåŠ å…¥åˆ°C3ä¸­
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(CABottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

class SPPCSPC(nn.Module):
    # CSP https://github.com/WongKinYiu/CrossStagePartialNetworks
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5, k=(5, 9, 13)):
        super(SPPCSPC, self).__init__()
        c_ = int(2 * c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(c_, c_, 3, 1)
        self.cv4 = Conv(c_, c_, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])
        self.cv5 = Conv(4 * c_, c_, 1, 1)
        self.cv6 = Conv(c_, c_, 3, 1)
        self.cv7 = Conv(2 * c_, c2, 1, 1)

    def forward(self, x):
        x1 = self.cv4(self.cv3(self.cv1(x)))
        y1 = self.cv6(self.cv5(torch.cat([x1] + [m(x1) for m in self.m], 1)))
        y2 = self.cv2(x)
        return self.cv7(torch.cat((y1, y2), dim=1))

class SPPFCSPC(nn.Module):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5, k=5):
        super(SPPFCSPC, self).__init__()
        c_ = int(2 * c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(c_, c_, 3, 1)
        self.cv4 = Conv(c_, c_, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)
        self.cv5 = Conv(4 * c_, c_, 1, 1)
        self.cv6 = Conv(c_, c_, 3, 1)
        self.cv7 = Conv(2 * c_, c2, 1, 1)

    def forward(self, x):
        x1 = self.cv4(self.cv3(self.cv1(x)))
        x2 = self.m(x1)
        x3 = self.m(x2)
        y1 = self.cv6(self.cv5(torch.cat((x1,x2,x3, self.m(x3)),1)))
        y2 = self.cv2(x)
        return self.cv7(torch.cat((y1, y2), dim=1))

#----------------------------------------------ä¿®æ”¹å·ç§¯å—----------------------------------------
class SCConv(nn.Module):
    # åˆ©ç”¨è‡ªæ ¡æ­£å·ç§¯å–ä»£åŸæ¥ç‰¹å¾æå–ç½‘ç»œä¸­çš„å¸¸è§„å·ç§¯  è§£å†³å°ç›®æ ‡é—®é¢˜ï¼ˆ2020ï¼‰
    # å°ç›®æ ‡ç”±äºæºå¸¦ä¿¡æ¯å°‘å¯¼è‡´ç‰¹å¾è¡¨è¾¾èƒ½åŠ›è¾ƒå¼±ï¼Œç»è¿‡å¤šå±‚æ¬¡çš„å·ç§¯æ“ä½œåèƒ½æå–åˆ°çš„ç‰¹å¾è¾ƒå°‘ï¼Œå› æ­¤æ£€æµ‹å›°éš¾ã€‚åˆ©ç”¨è‡ªæ ¡æ­£å·ç§¯å–ä»£ç‰¹å¾æå–ç½‘ç»œä¸­çš„å¸¸è§„å·ç§¯ï¼Œä»¥æ‰©å±•æ„Ÿå—é‡ä¸°å¯Œè¾“å‡ºï¼Œè¿›è€Œå¼ºåŒ–å¯¹å¼±ç‰¹å¾çš„æå–èƒ½åŠ›ã€‚
    # http://mftp.mmcheng.net/Papers/20cvprSCNet.pdf
    # https://github.com/MCG-NKU/SCNet
    # åœ¨ä¸è°ƒæ•´æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹æ”¹è¿›CNNçš„åŸºæœ¬å·ç§¯ç‰¹å¾è½¬æ¢è¿‡ç¨‹ã€‚æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªæ ¡æ­£å·ç§¯ï¼Œè¯¥å·ç§¯å¯ä»¥é€šè¿‡ç‰¹å¾çš„å†…åœ¨é€šä¿¡è¾¾åˆ°æ‰©å¢å·ç§¯æ„Ÿå—é‡çš„ç›®çš„ï¼Œè¿›è€Œå¢å¼ºè¾“å‡ºç‰¹å¾çš„å¤šæ ·æ€§ã€‚ä¸åŒäºæ ‡å‡†å·ç§¯é‡‡ç”¨å°å°ºå¯¸æ ¸ï¼ˆä¾‹å¦‚3Ã—3
    #å·ç§¯ï¼‰åŒæ—¶èåˆç©ºé—´ç»´åº¦åŸŸä¸é€šé“ç»´åº¦çš„ä¿¡æ¯ï¼ŒSCConvå¯ä»¥é€šè¿‡è‡ªæ ¡æ­£æ“ä½œè‡ªé€‚åº”åœ°åœ¨æ¯ä¸ªç©ºé—´ä½ç½®å‘¨å›´å»ºç«‹äº†è¿œç¨‹ç©ºé—´å’Œé€šé“é—´ä¾å­˜å…³ç³»ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥å¸®åŠ©CNNç”Ÿæˆæ›´å…·åˆ¤åˆ«èƒ½åŠ›çš„ç‰¹å¾è¡¨è¾¾ï¼Œå› å…¶å…·æœ‰æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚

    def __init__(self, c1, c2, stride, groups=1, dilation=1, pooling_r=4):
        super(SCConv, self).__init__()
        self.k2 = nn.Sequential(
                    nn.AvgPool2d(kernel_size=pooling_r, stride=pooling_r),
                    nn.Conv2d(c1, c1, kernel_size=3, stride=1,
                                padding=autopad(3, None), dilation=dilation,
                                groups=groups, bias=False),
                    nn.BatchNorm2d(c1),
                    )
        self.k3 = nn.Sequential(
                    nn.Conv2d(c1, c1, kernel_size=3, stride=1,
                                padding=autopad(3, None), dilation=dilation,
                                groups=groups, bias=False),
                    nn.BatchNorm2d(c1),
                    )
        self.k4 = nn.Sequential(
                    nn.Conv2d(c1, c2, kernel_size=3, stride=stride,
                                padding=autopad(3, None), dilation=dilation,
                                groups=groups, bias=False),
                    nn.BatchNorm2d(c2),
                    )
    def forward(self, x):
        identity = x
        # F.interpolate åˆ©ç”¨æ’å€¼çš„æ–¹å¼è¿›è¡Œæ•°ç»„é‡‡æ ·ï¼Œè¾“å‡ºç©ºé—´çš„å¤§å°ï¼ˆk2å…ˆä¸‹é‡‡æ ·ï¼Œç„¶åå†åˆ©ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·ï¼‰
        y_ = F.interpolate(self.k2(x), identity.size()[2:])
        y_ = torch.add(identity, y_)
        out = torch.sigmoid(y_) # sigmoid(identity + k2)
        out = torch.mul(self.k3(x), out) # k3 * sigmoid(identity + k2)
        out = self.k4(out) # k4
        return out

class GnConv(nn.Module):  # gnconvæ¨¡å—
    # é€’å½’é—¨å·ç§¯ï¼ˆæ€§èƒ½ä¼˜äºswin transformerå’Œconvnextï¼‰
    # CNNå…·æœ‰å¹³ç§»ä¸å˜æ€§å’Œå±€éƒ¨æ€§ï¼Œç¼ºä¹å…¨å±€å»ºæ¨¡é•¿è·ç¦»å»ºæ¨¡èƒ½åŠ›ï¼Œå¼•å…¥è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ¡†æ¶Transformeræ¥å½¢æˆCNN+Transformeræ¶æ„
    def __init__(self, c1, c2, ksize=1, stride=1, order=5, gflayer=None, h=14, w=8, s=1.0):
        super().__init__()
        self.order = order
        self.dims = [c1 // 2 ** i for i in range(order)]
        self.dims.reverse()
        self.proj_in = nn.Conv2d(c1, 2 * c1, 1)

        if gflayer is None:
            self.dwconv = get_dwconv(sum(self.dims), 7, True)
        else:
            self.dwconv = gflayer(sum(self.dims), h=h, w=w)

        self.proj_out = Conv(c1, c2, ksize, stride)

        self.pws = nn.ModuleList(
            [nn.Conv2d(self.dims[i], self.dims[i + 1], 1) for i in range(order - 1)]
        )

        self.scale = s

        #print('[gconv]', order, 'order with dims=', self.dims, 'scale=%.4f' % self.scale)

    def forward(self, x, mask=None, dummy=False):
        B, C, H, W = x.shape

        fused_x = self.proj_in(x)
        pwa, abc = torch.split(fused_x, (self.dims[0], sum(self.dims)), dim=1)

        dw_abc = self.dwconv(abc) * self.scale

        dw_list = torch.split(dw_abc, self.dims, dim=1)
        x = pwa * dw_list[0]

        for i in range(self.order - 1):
            x = self.pws[i](x) * dw_list[i + 1]

        x = self.proj_out(x)

        return x

def get_dwconv(dim, kernel, bias):
    return nn.Conv2d(dim, dim, kernel_size=kernel, padding=(kernel - 1) // 2, bias=bias, groups=dim)

class HorBlock(nn.Module):# HorBlockæ¨¡å—
    r""" HorNet block yoloair
    """
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6, gnconv=GnConv):
        super().__init__()

        self.norm1 = LayerNorm(dim, eps=1e-6, data_format='channels_first')
        self.gnconv = GnConv(dim, dim)
        self.norm2 = LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, 4 * dim)
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        self.gamma1 = nn.Parameter(layer_scale_init_value * torch.ones(dim),
                                    requires_grad=True) if layer_scale_init_value > 0 else None
        self.gamma2 = nn.Parameter(layer_scale_init_value * torch.ones((dim)),
                                    requires_grad=True) if layer_scale_init_value > 0 else None
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        B, C, H, W  = x.shape # [512]
        if self.gamma1 is not None:
            gamma1 = self.gamma1.view(C, 1, 1)
        else:
            gamma1 = 1
        x = x + self.drop_path(gamma1 * self.gnconv(self.norm1(x)))
        input = x
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)
        x = self.norm2(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma2 is not None:
            x = self.gamma2 * x
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)
        x = input + self.drop_path(x)

        return x

class LayerNorm(nn.Module):
    r""" LayerNorm that supports two data formats: channels_last (default) or channels_first.
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs
    with shape (batch_size, channels, height, width).
    """

    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ["channels_last", "channels_first"]:
            raise NotImplementedError
        self.normalized_shape = (normalized_shape,)

    def forward(self, x):
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == "channels_first":
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x
# ä½¿ç”¨C3HBä¼šé€ æˆæ¢¯åº¦æ¶ˆå¤±é—®é¢˜(å‡å°‘é‡å¤æ¬¡æ•°)
class C3HB(nn.Module):
    # CSP HorBlock with 3 convolutions by iscyy/yoloair
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)
        self.m = nn.Sequential(*(HorBlock(c_) for _ in range(n)))
    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))

class C3GhostV2(C3):
    # C3GV2 module with GhostV2(for iscyy)
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.c1_ = 16
        self.c2_ = 16 * e
        c_ = int(c2 * e)
        self.m = nn.Sequential(*(Ghostblockv2(c_, self.c1_, c_) for _ in range(n)))

# spd(2022)æ›¿æ¢è‡ªæ ¡æ­£å·ç§¯(2020å¹´)
class space_to_depth(nn.Module):
    # Changing the dimension of the Tensor
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
         return torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1)

class SM(nn.Module):
    # Changing the dimension of the Tensor
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
         return torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1)

class MP(nn.Module):
    def __init__(self, k=2):
        super(MP, self).__init__()
        self.m = nn.MaxPool2d(kernel_size=k, stride=k)

    def forward(self, x):
        return self.m(x)

"""class SMMConv(nn.Module):
    def __init__(self, c1, c2):
        super(SMMConv, self).__init__()
        c_ = int(c2/2)
        self.cv1 = Conv(c1, c2, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(c_, c2, 3, 1)
        self.sm = SM()
        self.mp  = MP()
    def forward(self, x):
        x_1 = self.mp(x)
        x_1 = self.cv1(x_1)

        x_2 = self.cv2(x)
        x_2 = self.cv3(x_2)
        x_2 = self.sm(x_2)
        return torch.cat([x_2, x_1], 1)"""
class SMMConv(nn.Module):
    def __init__(self, c1, c2):
        super(SMMConv, self).__init__()
        c_ = int(c1/2)
        self.cv1 = Conv(c1, c_, 3, 1)
        self.cv2 = Conv(c1, c_, 5, 1)
        self.sm = SM()
    def forward(self, x):
        x_1 = self.cv1(x)
        x_2 = self.cv2(x)
        x_3 = torch.cat([x_1,x_2],1)
        x_4 = self.sm(x_3)

        return x_4
class DMMConv2(nn.Module):
    def __init__(self, c1, c2):
        super(DMMConv2, self).__init__()
        self.cv1 = Conv(c1, c2, 1, 1)

        self.sm = SM()
        self.mp  = MP()
    def forward(self, x):
        x_1 = self.mp(x)
        x_1 = self.cv1(x_1)

        x_2 = self.sm(x)

        return torch.cat([x_2, x_1], 1)

class DMMConv(nn.Module):
    def __init__(self, c1, c2):
        super(DMMConv, self).__init__()
        self.cv1 = Conv(c1, c2, 1, 1)
        self.cv2 = Conv(c1, c2, 3, 1)
        self.sm = SM()
        self.mp  = MP()
    def forward(self, x):
        x_1 = self.mp(x)
        x_1 = self.cv1(x_1)
        x_2 = self.cv2(x)
        x_2 = self.sm(x_2)

        return torch.cat([x_2, x_1], 1)

class DMConv(nn.Module):
    def __init__(self, c1, c2):
        super(DMConv, self).__init__()
        self.cv1 = Conv(c1, c2, 3, 1)
        self.sm = SM()
    def forward(self, x):
        x = self.cv1(x)
        x = self.sm(x)

        return x

class DMMixConv2d(nn.Module):
    # MixConv2d æ··åˆæ·±åº¦å·ç§¯å°±æ˜¯ä½¿ç”¨ä¸åŒå¤§å°çš„å·ç§¯æ ¸å¯¹æ·±åº¦å·ç§¯çš„ä¸åŒchannelåˆ†ç»„å¤„ç†ã€‚ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ åˆ†ç»„æ·±åº¦å·ç§¯ + Inceptionç»“æ„ çš„å¤šç§å·ç§¯æ ¸æ··ç”¨ã€‚
    # Mixed Depth-wise Conv https://arxiv.org/abs/1907.09595
    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):  # ch_in, ch_out, kernel, stride, ch_strategy
        """
                :params c1: è¾“å…¥feature mapçš„é€šé“æ•°
                :params c2: è¾“å‡ºçš„feature mapçš„é€šé“æ•°ï¼ˆè¿™ä¸ªå‡½æ•°çš„å…³é”®ç‚¹å°±æ˜¯å¯¹c2è¿›è¡Œåˆ†ç»„ï¼‰
                :params k: æ··åˆçš„å·ç§¯æ ¸å¤§å° å…¶å®è®ºæ–‡é‡Œæ˜¯[3, 5, 7...]ç”¨çš„æ¯”è¾ƒå¤šçš„
                :params s: æ­¥é•¿ stride
                :params equal_ch: é€šé“åˆ’åˆ†æ–¹å¼ æœ‰å‡ç­‰åˆ’åˆ†å’ŒæŒ‡æ•°åˆ’åˆ†ä¸¤ç§æ–¹å¼  é»˜è®¤æ˜¯å‡ç­‰åˆ’åˆ†"""
        super().__init__()
        n = len(k)  # number of convolutions
        if equal_ch:  # equal c_ per group å‡ç­‰åˆ’åˆ†é€šé“
            i = torch.linspace(0, n - 1E-6, c2).floor()  # c2 indices
            c_ = [(i == g).sum() for g in range(n)]  # intermediate channels
        else:  # equal weight.numel() per group æŒ‡æ•°åˆ’åˆ†é€šé“
            b = [c2] + [0] * n
            a = np.eye(n + 1, n, k=-1)
            a -= np.roll(a, 1, axis=1)
            a *= np.array(k) ** 2
            a[0] = 1
            c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b

        self.m = nn.ModuleList(
            [nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])
        self.bn = nn.BatchNorm2d(c2)
        self.act = nn.SiLU()

    def forward(self, x):
        return self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))
        #return x + self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))#

class BAM(C3):
    # C3 module with CABottleneck()
    # å°†CAåŠ å…¥åˆ°C3ä¸­
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(CABottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))